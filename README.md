# OpenShift on OpenShift Virtualization

> **_NOTE:_** All content below together with most of the scripts and manifests were generated by AI. 

This repository provides automated tooling and configurations for deploying OpenShift clusters on top of OpenShift Virtualization. It supports both Single Node OpenShift (SNO) and Multi Node OpenShift (MNO) deployments using Agent Based Installer (ABI) with virtual machines.

## Overview

The solution leverages OpenShift Virtualization to create virtual machines that host OpenShift clusters, providing a nested virtualization approach for OpenShift deployments. This is particularly useful for testing, development, and edge computing scenarios.

## Repository Structure

```
openshift-on-openshift-virtualization/
├── README.md                           # This file
├── pre-setup.sh                        # Sets up local network configuration
├── install-sno.sh                      # SNO installation script  
├── install-mno.sh                      # MNO installation script
├── abi-configs/                        # Agent Based Installer configurations
│   ├── sno100.yaml                     # SNO cluster configuration
│   ├── vacm1.yaml                      # MNO dual-stack cluster configuration (vacm1)
│   ├── vacm2.yaml                      # MNO single IPv4 cluster configuration (vacm2)
│   └── extra-manifests/                # Day2 operations manifests
│       ├── vacm1/                      # VACM1 day2 configurations
│       │   └── day2/
│       │       ├── local-storage/      # Local storage configuration
│       │       ├── odf/                # OpenShift Data Foundation
│       │       └── rhacm/              # Red Hat Advanced Cluster Management
│       └── vacm2/                      # VACM2 day2 configurations
│           └── day2/                   # [similar structure to vacm1]
└── virtual-machines/                   # VM definitions and networking
    ├── localnet.yaml                   # Local network bridge mapping
    ├── vsno/                           # SNO VM configurations
    │   └── sno100/                     # SNO100 cluster VMs
    │       ├── kustomization.yaml      # Kustomize configuration
    │       ├── ns.yaml                 # Namespace definition
    │       ├── network-attachment-definition.yaml
    │       └── vm-sno100.yaml          # SNO VM definition
    ├── vacm1/                          # VACM1 cluster VMs
    │   ├── kustomization.yaml          # Kustomize configuration
    │   ├── ns.yaml                     # Namespace definition
    │   ├── network-attachment-definition.yaml
    │   ├── vm-master1.yaml             # Master node 1 VM
    │   ├── vm-master2.yaml             # Master node 2 VM
    │   └── vm-master3.yaml             # Master node 3 VM
    └── vacm2/                          # VACM2 cluster VMs
        └── [similar structure to vacm1]
```

## Prerequisites

### Infrastructure Requirements
- OpenShift cluster with OpenShift Virtualization operator installed
- Sufficient compute resources for nested virtualization
- Network connectivity for VM management
- Storage for VM disks and ISO images

### Software Dependencies
- `oc` (OpenShift CLI)
- `virtctl` (KubeVirt CLI)
- `git` for cloning dependencies
- Web server for hosting ISO images (configured at `http://192.168.58.15/iso/`)

### External Dependencies
This repository depends on external tools:
- [sno-agent-based-installer](https://github.com/borball/sno-agent-based-installer) - For SNO deployments
- [mno-with-abi](https://github.com/borball/mno-with-abi) - For MNO deployments

## Configuration

### ABI Configuration Files

The `abi-configs/` directory contains cluster-specific configurations and day2 operations manifests:

#### Day2 Operations
The `extra-manifests/` subdirectory contains post-installation configurations for additional cluster features:
- **Local Storage**: Configures local volume discovery and storage classes for OpenShift Data Foundation
- **OpenShift Data Foundation (ODF)**: Storage cluster configurations for persistent storage
- **Red Hat Advanced Cluster Management (RHACM)**: Multi-cluster management configurations

#### SNO Configuration (`sno100.yaml`)
```yaml
cluster:
  domain: outbound.vz.bos2.lab
  name: sno100

host:
  interface: ens1f0
  hostname: sno100.outbound.vz.bos2.lab
  mac: de:ad:be:ff:80:00
  ipv4:
    enabled: true
    dhcp: false
    ip: 192.168.58.100
    dns:
      - 192.168.58.15
    gateway: 192.168.58.1
    prefix: 25
    machine_network_cidr: 192.168.58.0/25
  disk: /dev/vda

cpu:
  isolated: 4-23
  reserved: 0-3

proxy:
  enabled: false

ssh_key: ${HOME}/.ssh/id_rsa.pub
pull_secret: ${HOME}/pull-secret.json


iso:
  address: http://192.168.58.15/iso/sno100.iso
```

#### MNO Configuration Examples

##### Dual-Stack Configuration (`vacm1.yaml`)
VACM1 is configured as a dual-stack cluster supporting both IPv4 and IPv6:

```yaml
#dual-stack example
cluster:
  domain: outbound.vz.bos2.lab
  name: vacm1
  apiVIPs:
    - 192.168.58.50      # IPv4 API VIP
    - 2600:52:7:58::50   # IPv6 API VIP
  ingressVIPs:
    - 192.168.58.54      # IPv4 Ingress VIP
    - 2600:52:7:58::54   # IPv6 Ingress VIP

hosts:
  common:
    ipv4:
      enabled: true
      dhcp: false
      machine_network_cidr: 192.168.58.0/25
      dns: 192.168.58.15
      gateway: 192.168.58.1
    ipv6:
      enabled: true
      dhcp: false
      machine_network_cidr: 2600:52:7:58::/64
      dns: 2600:52:7:58::15
      gateway: 2600:52:7:58::1
    disk: /dev/vda

  masters:
    - hostname: master1.vacm1.outbound.vz.bos2.lab
      interface: ens1f0
      mac: de:ad:be:ff:20:00
      ipv4:
        ip: 192.168.58.51
      ipv6:
        ip: 2600:52:7:58::51
    # ... additional masters with both IPv4 and IPv6 addresses

day2:
  operators:
    odf:
      dual_stack: true              # Enable dual-stack for ODF
      storage_device_count: 3
```

##### Single IPv4 Configuration (`vacm2.yaml`)
VACM2 is configured as a single-stack IPv4-only cluster:

```yaml
#single-stack example
cluster:
  domain: outbound.vz.bos2.lab
  name: vacm2
  apiVIPs:
    - 192.168.58.70      # IPv4 API VIP only
  ingressVIPs:
    - 192.168.58.74      # IPv4 Ingress VIP only

hosts:
  common:
    ipv4:
      enabled: true
      dhcp: false
      machine_network_cidr: 192.168.58.0/25
      dns: 192.168.58.15
      gateway: 192.168.58.1
    ipv6:
      enabled: false       # IPv6 disabled
    disk: /dev/vda

  masters:
    - hostname: master1.vacm2.outbound.vz.bos2.lab
      interface: ens1f0
      mac: de:ad:be:ff:40:00
      ipv4:
        ip: 192.168.58.71  # IPv4 address only
    # ... additional masters with IPv4 addresses only

day2:
  operators:
    odf:
      dual_stack: false            # Disable dual-stack for ODF
      ip_family: IPv4              # Specify IPv4 family
      storage_device_count: 3
```

### Virtual Machine Specifications

#### SNO VM Configuration
- **CPU**: 8 cores, 1 socket, 2 threads
- **CPU Isolation**: Cores 4-23 isolated, 0-3 reserved (configurable)
- **Memory**: 24Gi
- **Storage**: 120Gi root disk + 100Gi data disk
- **Network**: Virtio interface with localnet networking
- **MAC Address**: de:ad:be:ff:80:00

#### MNO VM Configuration (per node)
- **CPU**: 8 cores, 1 socket, 2 threads  
- **Memory**: 16Gi
- **Storage**: 120Gi root disk + 100Gi data disk
- **Network**: Virtio interface with localnet networking

## Usage

### 1. Pre-setup
Set up the local network configuration:
```bash
./pre-setup.sh
```

This creates the necessary network bridge mappings for VM connectivity.

### 2. Deploy Single Node OpenShift (SNO)
```bash
./install-sno.sh <cluster-name>
```

Example:
```bash
./install-sno.sh sno100
```

This script:
1. Clones the `sno-agent-based-installer` repository
2. Copies the ABI configuration file
3. Generates the OpenShift ISO using `sno-iso.sh`
4. Copies the ISO to the web server location
5. Creates the VM using Kustomize
6. Powers on the VM using `virtctl`

**Features**:
- **Installation monitoring**: Tracks installation progress via Assisted Service API
- **Cluster readiness**: Waits for cluster operators to reach stable state
- **Progress reporting**: Shows installation percentage and status updates
- **Error handling**: Includes timeout and retry mechanisms

**Remaining TODO items for future enhancements**:
- Unmount the ISO from the VM

### 3. Deploy Multi Node OpenShift (MNO)
```bash
./install-mno.sh <cluster-name>
```

Examples:
```bash
# Deploy dual-stack cluster (VACM1)
./install-mno.sh vacm1

# Deploy single IPv4 cluster (VACM2)
./install-mno.sh vacm2
```

This script provides a complete automated deployment workflow:
1. Clones the `mno-with-abi` repository
2. Copies the ABI configuration file
3. Generates the OpenShift ISO using `mno-iso.sh`
4. Copies the ISO to the web server location
5. Creates all master VMs using Kustomize
6. Waits for all DataVolumes to be ready (up to 10 minutes)
7. Powers on all master VMs using `virtctl`
8. Monitors the installation progress via the Assisted Service API
9. Waits for cluster stability (5-minute minimum stable period)
10. Executes day2 operations (local storage, ODF, RHACM configurations)
11. Provides detailed progress reporting throughout the process

**Features**:
- **Automated monitoring**: Tracks installation progress and completion
- **DataVolume readiness**: Ensures VMs are ready before powering on
- **Cluster stability**: Waits for cluster to reach stable state
- **Day2 operations**: Automatically configures local storage, ODF, and RHACM
- **Progress reporting**: Shows installation percentage and status
- **Error handling**: Includes timeout and retry mechanisms

## Network Configuration

### Cluster Network Types

#### Dual-Stack Configuration (VACM1)
VACM1 is configured with dual-stack networking supporting both IPv4 and IPv6:

**Key Features:**
- **API VIPs**: Both IPv4 (192.168.58.50) and IPv6 (2600:52:7:58::50) endpoints
- **Ingress VIPs**: Both IPv4 (192.168.58.54) and IPv6 (2600:52:7:58::54) endpoints
- **Machine Networks**: 
  - IPv4: 192.168.58.0/25
  - IPv6: 2600:52:7:58::/64
- **DNS**: Dual-stack DNS servers (IPv4: 192.168.58.15, IPv6: 2600:52:7:58::15)
- **ODF Configuration**: `dual_stack: true` for storage cluster

#### Single IPv4 Configuration (VACM2)
VACM2 is configured with single-stack IPv4-only networking:

**Key Features:**
- **API VIPs**: IPv4 only (192.168.58.70)
- **Ingress VIPs**: IPv4 only (192.168.58.74)
- **Machine Network**: IPv4 only (192.168.58.0/25)
- **DNS**: IPv4 only (192.168.58.15)
- **IPv6**: Explicitly disabled (`enabled: false`)
- **ODF Configuration**: `dual_stack: false`, `ip_family: IPv4`

### Local Network Bridge Mapping
The `localnet.yaml` file configures OVN bridge mappings:
- Maps `localnet-network` to the `br-ex` bridge
- Applied to worker nodes for VM networking
- Supports both dual-stack and single-stack configurations

### Network Attachment Definition
Each cluster uses a `NetworkAttachmentDefinition` for VM networking:
- **Type**: `ovn-k8s-cni-overlay`
- **Topology**: `localnet`
- **CNI Version**: 0.3.1
- **Compatibility**: Works with both dual-stack and single IPv4 configurations

## VM Management

### Starting VMs
```bash
# SNO
virtctl start <cluster-name> -n <cluster-name>

# MNO
virtctl start master1 -n <cluster-name>
virtctl start master2 -n <cluster-name>
virtctl start master3 -n <cluster-name>
```

### Stopping VMs
```bash
virtctl stop <vm-name> -n <namespace>
```

### Accessing VMs
```bash
virtctl console <vm-name> -n <namespace>
```

## Customization

### Adding New Clusters
1. Create ABI configuration file in `abi-configs/`
2. Create VM definitions in `virtual-machines/`
3. Update MAC addresses and IP configurations
4. Ensure unique cluster names and namespaces
5. (Optional) Create day2 operations manifests in `abi-configs/extra-manifests/<cluster-name>/day2/`

### Configuring Day2 Operations
Day2 operations are automatically executed for MNO clusters if the corresponding manifests exist:

#### Local Storage Configuration
- `01-local-vole-discovery.yaml`: Enables automatic discovery of local storage devices
- `02-odf-localdisk.yaml`: Creates LocalVolumeSet for OpenShift Data Foundation

#### OpenShift Data Foundation (ODF)
- `03-odf-storagecluster.yaml.j2`: Configures the storage cluster with customizable parameters
  - `storage_device_count`: Number of storage devices (default: 3)
  - `dual_stack`: Enable dual-stack networking (true for VACM1, false for VACM2)
  - `ip_family`: IP family configuration (not used when dual_stack is true, set to IPv4 for VACM2)

#### Red Hat Advanced Cluster Management
- Additional RHACM configurations can be placed in the `rhacm/` subdirectory

### Modifying VM Resources
Edit the VM YAML files to adjust:
- CPU cores and memory
- Storage sizes
- Network interfaces
- Boot order and devices

### Network Customization
Modify network configurations in:
- `localnet.yaml` for bridge mappings
- `network-attachment-definition.yaml` for VM networking
- ABI configs for cluster networking

## Troubleshooting

### Common Issues
1. **ISO not found**: Ensure the web server is running and ISO files are accessible
2. **VM creation fails**: Check resource availability and storage classes
3. **Network connectivity**: Verify bridge mappings and NetworkAttachmentDefinitions
4. **Boot issues**: Check VM console logs and boot order configuration
5. **DataVolume timeout**: If DataVolumes don't become ready within 10 minutes, check storage provisioning
6. **Installation monitoring fails**: Verify SSH access to the rendezvous node and assisted service availability
7. **Cluster stability timeout**: The MNO script waits for 5-minute stability; check cluster health during this phase
8. **Day2 operations fail**: Check if extra-manifests directory exists and manifests are valid YAML
9. **ODF storage issues**: Verify local storage devices are available and meet minimum size requirements (100Gi)
10. **Dual-stack networking issues**: For VACM1, ensure IPv6 connectivity and proper DNS resolution for both IP families
11. **Single-stack networking issues**: For VACM2, verify IPv4 connectivity and that IPv6 is properly disabled

### Debugging Commands
```bash
# Check VM status
oc get vms -n <namespace>

# Check VM details
oc describe vm <vm-name> -n <namespace>

# Check DataVolume status
oc get dv -n <namespace>

# Check network configuration
oc get network-attachment-definitions -n <namespace>

# Access VM console
virtctl console <vm-name> -n <namespace>

# Monitor cluster installation status (for MNO)
oc adm wait-for-stable-cluster --minimum-stable-period=5m

# Check assisted service logs (on rendezvous node)
ssh core@<rendezvous-ip> journalctl -u assisted-service

# Check day2 operations status
oc get localvolumediscovery -n openshift-local-storage
oc get localvolumeset -n openshift-local-storage
oc get storagecluster -n openshift-storage

# Check ODF operator status
oc get csv -n openshift-storage | grep odf
oc get pods -n openshift-storage

# Check local storage operator
oc get csv -n openshift-local-storage | grep local-storage
oc get pods -n openshift-local-storage

# Check network configuration
oc get nodes -o wide  # Shows both IPv4 and IPv6 addresses for dual-stack
oc get services -A | grep -E "(api|router)"  # Check API and ingress services
oc get network.config.openshift.io cluster -o yaml  # Check cluster network config

# For dual-stack clusters (VACM1)
ping6 2600:52:7:58::50  # Test IPv6 API VIP connectivity
nslookup api.vacm1.outbound.vz.bos2.lab 2600:52:7:58::15  # Test IPv6 DNS

# For single IPv4 clusters (VACM2)
ping 192.168.58.70  # Test IPv4 API VIP connectivity
nslookup api.vacm2.outbound.vz.bos2.lab 192.168.58.15  # Test IPv4 DNS
```

## Dependencies and Requirements

### OpenShift Cluster Requirements
- OpenShift Virtualization operator installed
- Sufficient worker nodes with virtualization capabilities
- CNV (Container Native Virtualization) configured
- Storage classes for VM disks

### Network Requirements
- Bridge network configuration on worker nodes
- Access to ISO hosting web server
- Proper DNS resolution for cluster domains

### Resource Planning
- **SNO**: ~24Gi RAM, 8 vCPUs, 220Gi storage per cluster
- **MNO**: ~48Gi RAM, 24 vCPUs, 660Gi storage per cluster (3 masters)

## Execution Example

### MNO Dual-Stack Cluster (VACM1)

For a complete detailed execution log of the dual-stack VACM1 cluster deployment, including day2 operations, see [logs/vacm1.md](logs/vacm1.md).

The log shows the complete deployment process including:
- ISO generation and VM creation
- Installation monitoring with progress tracking  
- Cluster stability verification
- Day2 operations execution (local storage, ODF, RHACM configurations)
- Final cluster status with all operators installed

